# CodeGrok MCP

MCP server for semantic code search. No CLI, no LLM - pure indexing and search service via MCP protocol.

## Structure

```
src/codegrok_mcp/
├── core/models.py           # Symbol, ParsedFile, CodebaseIndex + Memory, MemoryType
├── core/interfaces.py       # IParser abstract interface
├── parsers/treesitter_parser.py  # Multi-language AST parsing (9 languages)
├── parsers/language_configs.py   # Tree-sitter node → SymbolType mappings
├── indexing/embedding_service.py # EmbeddingServiceBase + Native/Ollama implementations
├── indexing/parallel_indexer.py  # Thread-pool file processing
├── indexing/source_retriever.py  # Code indexing + semantic search
├── indexing/memory_retriever.py  # Memory storage + semantic recall
└── mcp/
    ├── state.py             # Global singleton (SourceRetriever + MemoryRetriever)
    └── server.py            # FastMCP server (8 tools)
```

## MCP Tools (8 total)

### Code Search
| Tool | Purpose |
|------|---------|
| `learn` | Index codebase. Modes: `auto` (incremental), `full`, `load_only`. **Required first.** |
| `get_sources` | Semantic search. Returns ranked code chunks with filters (language, symbol_type) |
| `get_stats` | Index statistics (files, symbols, chunks) |
| `list_supported_languages` | Supported language extensions |

### Memory Layer
| Tool | Purpose |
|------|---------|
| `remember` | Store memories (conversation, status, decision, preference, doc, note) |
| `recall` | Semantic search over memories. Filters: type, tags, time_range |
| `forget` | Delete memories by ID, type, tags, or age. **Destructive.** |
| `memory_stats` | Memory statistics by type |

## Key Constants

- **Embedding Provider**: Local (SentenceTransformers) or Ollama API
- **Embedding Models**:
  - Local: `nomic-ai/CodeRankEmbed` (768 dims, 8192 max tokens, default)
  - Ollama: Configurable via `CODEGROK_OLLAMA_MODEL`
- **Chunk Strategy**: Symbol-based (each function/class/method = 1 chunk)
- **Max Chunk Size**: 4000 chars (~1000-1300 tokens)
- **Storage**: `.codegrok/` (chromadb/ + metadata.json + memory_metadata.json)
- **Parallelism**: CPU count - 1 workers (min 1, max 32)
- **Memory TTLs**: session (24h), day, week, month, permanent

## Commands

```bash
pip install -e .          # Install
codegrok-mcp              # Run server
pytest                    # Test
black src/ --line-length 100  # Format
mypy src/                 # Type check
```

## Gotchas

1. **State is global singleton** - `state.py` holds SourceRetriever + MemoryRetriever across MCP calls
2. **Incremental reindex uses SHA256** - File hash comparison, not mtime
3. **ChromaDB collections**: `codebase_chunks` (code) and `memories` (memory layer)
4. **No LLM code** - Removed from parent CodeGrok; source_retriever.py has no ask/rerank methods
5. **Tree-sitter node names vary by language** - language_configs.py normalizes them
6. **Embedding is cached** - LRU(1000) + batch processing in embedding_service.py
7. **Memory tags stored as CSV** - ChromaDB doesn't support list metadata; tags joined with commas
8. **All tools require `learn` first** - Except `list_supported_languages` (static data)
9. **Embedding provider configurable via env vars** - `CODEGROK_EMBEDDING_PROVIDER=local|ollama`
10. **Ollama requires CODEGROK_OLLAMA_DIMENSIONS** - Must specify embedding dimensions when using Ollama

## Adding Languages

1. Verify tree-sitter-languages support
2. Add node mappings to `language_configs.py`
3. Add extensions to `SUPPORTED_LANGUAGES` in treesitter_parser.py

## Adding MCP Tools

1. Add `@mcp.tool` function in `server.py`
2. Use `state.get_state()` for retriever access
3. Add `ToolAnnotations` (readOnlyHint, destructiveHint, idempotentHint)
4. Return dict with results or raise `ToolError` on failure

## Memory Types

| Type | Use Case |
|------|----------|
| `conversation` | Chat history, Q&A exchanges |
| `status` | Project blockers, progress notes |
| `decision` | Architecture choices, rationale |
| `preference` | User coding style, settings |
| `doc` | Documentation snippets |
| `note` | General notes, TODOs |
